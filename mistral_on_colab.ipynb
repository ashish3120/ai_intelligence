{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Run Mistral LLM (Ollama) in Google Colab\n",
                "\n",
                "This notebook allows you to run the Mistral 7B LLM on Google Colab's T4 GPU for free."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Install Ollama\n",
                "!curl -fsSL https://ollama.com/install.sh | sh"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Start Ollama Server (Background)\n",
                "import subprocess\n",
                "import time\n",
                "\n",
                "print(\"Starting Ollama server...\")\n",
                "process = subprocess.Popen([\"ollama\", \"serve\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
                "time.sleep(5)  # Wait for it to boot\n",
                "print(\"Ollama server started.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Pull Mistral 7B Model\n",
                "print(\"Downloading Mistral 7B (this takes 2-3 mins)...\")\n",
                "!ollama pull mistral:7b-instruct-q4_K_M"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. Install LangChain for easy interaction\n",
                "!pip install langchain-ollama"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 5. Run Inference\n",
                "from langchain_ollama import OllamaLLM\n",
                "\n",
                "llm = OllamaLLM(model=\"mistral:7b-instruct-q4_K_M\")\n",
                "\n",
                "response = llm.invoke(\"Explain quantum computing to a 5 year old.\")\n",
                "print(response)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}